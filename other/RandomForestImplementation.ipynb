{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "workingdir = r\"C:\\Users\\Renato\\Dropbox\\Personal\\Graduate School\\Princeton\\Conferences\\SAMSI Workshop 2017 - Research Triangle Park NC\\Code\"\n",
    "Datapath = workingdir + '\\\\Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training and Testing Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "import random as random\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "df=pd.read_csv(Datapath + '\\\\Data_HotOneEncodedColony.csv', sep=',')\n",
    "#col_inputs = ['A12_1', 'A12_2', 'M12_1', 'M12_6', 'ML5126', 'HeadToThorax_mm','Corpulence_mm','Dist']\n",
    "col_inputs = ['HeadToThorax_mm','Corpulence_mm','Dist']\n",
    "INPUTS = df[col_inputs].values\n",
    "OUTPUT = df[['Activity']].values.ravel()\n",
    "\n",
    "Activity = ['b', 'bc', 'f', 'fd', 'go', 'i', 'og', 'sg', 't', 'wi', 'wo'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "dfn.hist()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree (75% training, 25% testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# group labels to reduce overall number of tasks\n",
    "dfMod = df.copy()\n",
    "# Group 0: b (Nest Building)\n",
    "# dfMod.loc[dfMod['Activity'] == 0, 'Activity'] = 0\n",
    "# Group 1: bc (Brood Care)\n",
    "# dfMod.loc[dfMod['Activity'] == 1, 'Activity'] = 1\n",
    "# Group 2: wo+f (Outside)\n",
    "dfMod.loc[(dfMod['Activity'] == 10) | (dfMod['Activity'] == 2), 'Activity'] = 2\n",
    "# Group 3: sg+fd (Selfish)\n",
    "dfMod.loc[(dfMod['Activity'] == 7) | (dfMod['Activity'] == 3), 'Activity'] = 3\n",
    "# Group 4: t+go+og (Interaction)\n",
    "dfMod.loc[(dfMod['Activity'] == 8) | (dfMod['Activity'] == 4) | (dfMod['Activity'] == 6), 'Activity'] = 4\n",
    "# Group 5: i (Inactive)\n",
    "dfMod.loc[(dfMod['Activity'] == 5) | (dfMod['Activity'] == 9), 'Activity'] = 5\n",
    "ModActivity = ['NB','BC','O','S','A','I']\n",
    "\n",
    "ModINPUTS = dfMod[col_inputs].values\n",
    "ModOUTPUT = dfMod[['Activity']].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global F1: 0.94894, zero one loss: 0.05106\n",
      "Depth: 25, Node Count: 7839\n",
      "A: 0.93374\n",
      "I: 0.95847\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define tree parameters\n",
    "# criterion='gini'\n",
    "# splitter='best'\n",
    "# max_depth=None\n",
    "# min_samples_split=2\n",
    "# min_samples_leaf=1\n",
    "# min_weight_fraction_leaf=0.0\n",
    "# max_features=None\n",
    "# random_state=None\n",
    "# max_leaf_nodes=None\n",
    "# min_impurity_split=1e-07\n",
    "# class_weight=None\n",
    "# presort=False\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(ModINPUTS, ModOUTPUT, test_size=0.25, stratify=OUTPUT)\n",
    "# X_train, X_test = INPUTS[train_index,:], INPUTS[test_index,:]\n",
    "# Y_train, Y_test = OUTPUT[train_index], OUTPUT[test_index]\n",
    "# Train Classifier\n",
    "clf = tree.DecisionTreeClassifier(criterion='gini', splitter='best', min_samples_split=2,\n",
    "                                      min_samples_leaf=5, max_depth = 25)\n",
    "clf.fit(X_train, Y_train)\n",
    "# Test Classifier\n",
    "Y_pred = clf.predict(X_test)\n",
    "# Measure Classification Error\n",
    "# zero_one_loss returns fractions of misclasssifications (0 is perfect)\n",
    "f1 = f1_score(Y_test,Y_pred,average=None)\n",
    "avgf1 = f1_score(Y_test,Y_pred,average='micro')    \n",
    "error = zero_one_loss(Y_pred,Y_test)\n",
    "print(\"Global F1: {0:.5f}, zero one loss: {1:.5f}\".format(avgf1, error))\n",
    "print(\"Depth: {0}, Node Count: {1}\".format(clf.tree_.max_depth,clf.tree_.node_count))\n",
    "print('\\n'.join('{0}: {1:.5f}'.format(ModActivity[idx],k) for idx,k in enumerate(f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PLOT TREE\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, feature_names=col_inputs,class_names=Activity,filled=True, rounded=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_svg(\"tree_MaxDepth10.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=5,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive Grid Search to Find Optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'max_depth': 15, 'max_features': 'auto', 'n_estimators': 100}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.739 (+/-0.007) for {'max_depth': 15, 'max_features': 'auto', 'n_estimators': 5}\n",
      "0.746 (+/-0.014) for {'max_depth': 15, 'max_features': 'auto', 'n_estimators': 10}\n",
      "0.752 (+/-0.009) for {'max_depth': 15, 'max_features': 'auto', 'n_estimators': 15}\n",
      "0.750 (+/-0.007) for {'max_depth': 15, 'max_features': 'auto', 'n_estimators': 20}\n",
      "0.754 (+/-0.014) for {'max_depth': 15, 'max_features': 'auto', 'n_estimators': 30}\n",
      "0.756 (+/-0.007) for {'max_depth': 15, 'max_features': 'auto', 'n_estimators': 50}\n",
      "0.758 (+/-0.005) for {'max_depth': 15, 'max_features': 'auto', 'n_estimators': 100}\n",
      "0.756 (+/-0.005) for {'max_depth': 15, 'max_features': 'auto', 'n_estimators': 200}\n",
      "0.738 (+/-0.013) for {'max_depth': 15, 'max_features': 'sqrt', 'n_estimators': 5}\n",
      "0.751 (+/-0.010) for {'max_depth': 15, 'max_features': 'sqrt', 'n_estimators': 10}\n",
      "0.753 (+/-0.011) for {'max_depth': 15, 'max_features': 'sqrt', 'n_estimators': 15}\n",
      "0.749 (+/-0.009) for {'max_depth': 15, 'max_features': 'sqrt', 'n_estimators': 20}\n",
      "0.755 (+/-0.006) for {'max_depth': 15, 'max_features': 'sqrt', 'n_estimators': 30}\n",
      "0.754 (+/-0.012) for {'max_depth': 15, 'max_features': 'sqrt', 'n_estimators': 50}\n",
      "0.755 (+/-0.007) for {'max_depth': 15, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "0.758 (+/-0.006) for {'max_depth': 15, 'max_features': 'sqrt', 'n_estimators': 200}\n",
      "0.739 (+/-0.011) for {'max_depth': 15, 'max_features': 'log2', 'n_estimators': 5}\n",
      "0.745 (+/-0.005) for {'max_depth': 15, 'max_features': 'log2', 'n_estimators': 10}\n",
      "0.748 (+/-0.016) for {'max_depth': 15, 'max_features': 'log2', 'n_estimators': 15}\n",
      "0.751 (+/-0.011) for {'max_depth': 15, 'max_features': 'log2', 'n_estimators': 20}\n",
      "0.751 (+/-0.010) for {'max_depth': 15, 'max_features': 'log2', 'n_estimators': 30}\n",
      "0.753 (+/-0.006) for {'max_depth': 15, 'max_features': 'log2', 'n_estimators': 50}\n",
      "0.754 (+/-0.006) for {'max_depth': 15, 'max_features': 'log2', 'n_estimators': 100}\n",
      "0.753 (+/-0.005) for {'max_depth': 15, 'max_features': 'log2', 'n_estimators': 200}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.83      0.84      0.84      1357\n",
      "        1.0       0.81      0.82      0.82      8581\n",
      "        2.0       0.79      0.81      0.80       558\n",
      "        3.0       0.74      0.69      0.71      1085\n",
      "        4.0       0.78      0.66      0.71      1491\n",
      "        5.0       0.92      0.98      0.95     54728\n",
      "        6.0       0.82      0.75      0.78      2464\n",
      "        7.0       0.72      0.48      0.57      2988\n",
      "        8.0       0.81      0.54      0.65      1018\n",
      "        9.0       0.76      0.68      0.71     13733\n",
      "       10.0       0.84      0.80      0.82       999\n",
      "\n",
      "avg / total       0.87      0.87      0.87     89002\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Split the dataset into two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(INPUTS, OUTPUT, test_size=0.5, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = { \n",
    "    'n_estimators': [5, 10, 15, 20, 30, 50, 100, 200],\n",
    "    'max_depth': [15],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "scores = ['f1']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(RFC(), tuned_parameters, cv=5, scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Cross Validation with parameters found above that maximize F1 metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] Global F1: 0.86617, zero one loss: 0.13383\n",
      "[Fold 2] Global F1: 0.86504, zero one loss: 0.13496\n",
      "[Fold 3] Global F1: 0.86013, zero one loss: 0.13987\n",
      "[Fold 4] Global F1: 0.86147, zero one loss: 0.13853\n",
      "[Fold 5] Global F1: 0.86692, zero one loss: 0.13308\n",
      "[Fold 6] Global F1: 0.86534, zero one loss: 0.13466\n",
      "[Fold 7] Global F1: 0.85028, zero one loss: 0.14972\n",
      "[Fold 8] Global F1: 0.87088, zero one loss: 0.12912\n",
      "[Fold 9] Global F1: 0.85389, zero one loss: 0.14611\n",
      "[Fold 10] Global F1: 0.86277, zero one loss: 0.13723\n",
      "------------------------Computational time: 45.06 seconds -------------------------------\n",
      "Average zero one loss: 0.13771s +- 0.00590\n",
      "Average Global. F1: 0.86229s +- 0.00590\n",
      "Average F1:\n",
      "b: 0.83205 +- 0.01553\n",
      "bc: 0.80552 +- 0.01458\n",
      "f: 0.81302 +- 0.01958\n",
      "fd: 0.64787 +- 0.02396\n",
      "go: 0.69037 +- 0.02735\n",
      "i: 0.93779 +- 0.00496\n",
      "og: 0.78665 +- 0.01803\n",
      "sg: 0.56587 +- 0.02172\n",
      "t: 0.60014 +- 0.02336\n",
      "wi: 0.68565 +- 0.01370\n",
      "wo: 0.83534 +- 0.01676\n",
      "------------------------FEATURE IMPORTANCE-----------------------------------------\n",
      "HeadToThorax_mm: 0.26934 +- 0.00416\n",
      "Corpulence_mm: 0.27508 +- 0.00410\n",
      "Dist: 0.45558 +- 0.00619\n"
     ]
    }
   ],
   "source": [
    "# A random forest is a meta estimator that fits a number of decision tree \n",
    "# classifiers on various sub-samples of the dataset and use averaging to improve\n",
    "# the predictive accuracy and control over-fitting. The sub-sample size is always\n",
    "# the same as the original input sample size but the samples are drawn with \n",
    "# replacement if bootstrap=True (default).\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from timeit import default_timer as timer\n",
    "# Define parameters of classifier\n",
    "nTrees = 100\n",
    "maxDepth = 15\n",
    "\n",
    "# Stratified K-Folds cross validation iterator\n",
    "# Provides train/test indices to split data in train test sets.\n",
    "# This cross-validation object is a variation of KFold that returns stratified folds. \n",
    "# The folds are made by preserving the percentage of samples for each class.\n",
    "# Define variables\n",
    "nfolds = 10 #Number of folds to use\n",
    "averageF1 = 0.0 #running avg f1\n",
    "averageError = 0.0 #running avg error\n",
    "\n",
    "# Define train/test indices\n",
    "skf = StratifiedKFold(n_splits=nfolds)\n",
    "skf.shuffle = True\n",
    "# Initialize lists\n",
    "avgf1 = []\n",
    "f1 = []\n",
    "error = []\n",
    "fi = [] # feature importance\n",
    "rf = [] # random forest list\n",
    "# Perform cross-validation\n",
    "start = timer()\n",
    "idx = 0;\n",
    "for train_index, test_index in skf.split(INPUTS,OUTPUT):\n",
    "    X_train, X_test = INPUTS[train_index,:], INPUTS[test_index,:]\n",
    "    Y_train, Y_test = OUTPUT[train_index], OUTPUT[test_index]\n",
    "    # Train Classifier\n",
    "    rf.append(RandomForestClassifier(n_estimators=nTrees,max_depth=maxDepth))\n",
    "    rf[idx].fit(X_train,Y_train)\n",
    "    # Test Classifier\n",
    "    Y_pred = rf[idx].predict(X_test)\n",
    "    # Measure Classification Error\n",
    "    # zero_one_loss returns fractions of misclasssifications (0 is perfect)\n",
    "    error.append(zero_one_loss(Y_test,Y_pred))\n",
    "    f1.append(f1_score(Y_test,Y_pred,average=None))\n",
    "    avgf1.append(f1_score(Y_test,Y_pred,average='micro'))\n",
    "    fi.append(rf[idx].feature_importances_)\n",
    "    print(\"[Fold {0}] Global F1: {1:.5f}, zero one loss: {2:.5f}\".format(idx+1, avgf1[idx], error[idx]))\n",
    "    # print('\\n'.join('{0}: {1:.5f}'.format(Activity[i],k) for i,k in enumerate(f1[idx])))\n",
    "    # print('\\n'.join('{0}: {1:.5f}'.format(col_inputs[i],k) for i,k in enumerate(fi[idx])))\n",
    "    idx +=1\n",
    "end = timer()\n",
    "print(\"------------------------Computational time: %4.2f%s -------------------------------\" % (end-start,' seconds'))\n",
    "print(\"Average zero one loss: {0:.5f}s +- {1:0.5f}\".format(np.mean(error),np.std(error)))\n",
    "print(\"Average Global. F1: {0:.5f}s +- {1:0.5f}\".format(np.mean(avgf1),np.std(avgf1)))\n",
    "print(\"Average F1:\")\n",
    "print('\\n'.join('{0}: {1:.5f} +- {2:.5f}'.format(Activity[if1],kf1[0],kf1[1]) for if1,kf1 in enumerate(zip(np.vstack(f1).mean(axis=0),np.vstack(f1).std(axis=0)))))\n",
    "print(\"------------------------FEATURE IMPORTANCE-----------------------------------------\")\n",
    "print('\\n'.join('{0}: {1:.5f} +- {2:.5f}'.format(col_inputs[ifi],kfi[0],kfi[1]) for ifi,kfi in enumerate(zip(np.vstack(fi).mean(axis=0),np.vstack(fi).std(axis=0)))))\n",
    "# print out graphviz tree visualization\n",
    "# tree0 = rf.estimators_[0];\n",
    "# tree.export_graphviz(tree0, out_file='tree.dot') \n",
    "# dot -Tps tree.dot -o outfile.ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot importance of features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm \n",
    "\n",
    "N = len(col_inputs) # number of features\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.05        # the width of the bars\n",
    "bars = []\n",
    "# Plot the feature importances of the forest\n",
    "fig, ax = plt.subplots()\n",
    "c=iter(cm.rainbow(np.linspace(0,1,len(rf))))\n",
    "for i in range(0,len(rf)):\n",
    "    importances = fi[i]\n",
    "    std = np.std([tree.feature_importances_ for tree in rf[i].estimators_],axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    bars.append(ax.bar(ind + width*i, importances[indices], width, color=next(c), yerr=std[indices]))\n",
    "\n",
    "    \n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Importance')\n",
    "ax.set_title('Feature Importance')\n",
    "ax.set_xticks(ind + width*5)\n",
    "ax.set_xticklabels(col_inputs[i] for i in indices)\n",
    "ax.set_ylim(0,1)\n",
    "ax.legend((b[0] for b in bars), (\"Fold {0}\".format(idx+1) for idx,k in enumerate(bars)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try One vs Rest Approach to find important features per task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define parameters of classifier\n",
    "nTrees = 20\n",
    "maxDepth = 15\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(INPUTS, OUTPUT, test_size=0.25, stratify=OUTPUT)\n",
    "clf = OneVsRestClassifier(RandomForestClassifier(n_estimators=nTrees,max_depth=maxDepth))\n",
    "clf.fit(INPUTS,OUTPUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm \n",
    "\n",
    "N = len(col_inputs) # number of features\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.05        # the width of the bars\n",
    "bars = []\n",
    "# Plot the feature importances of the forest\n",
    "fig, ax = plt.subplots()\n",
    "c=iter(cm.rainbow(np.linspace(0,1,len(Activity))))\n",
    "for i in range(0,len(Activity)):\n",
    "    rf = clf.estimators_[i]\n",
    "    importances = rf.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in rf.estimators_],axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    bars.append(ax.bar(ind + width*i, importances[indices], width, color=next(c), yerr=std[indices]))\n",
    "\n",
    "    \n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Importance')\n",
    "ax.set_title('Feature Importance per Task (OnevsRest)')\n",
    "ax.set_xticks(ind + width*5)\n",
    "ax.set_xticklabels(col_inputs[i] for i in indices)\n",
    "ax.set_ylim(0,1)\n",
    "ax.legend((b[0] for b in bars), Activity)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Define tree parameters\n",
    "# criterion='gini'\n",
    "# splitter='best'\n",
    "# max_depth=None\n",
    "# min_samples_split=2\n",
    "# min_samples_leaf=1\n",
    "# min_weight_fraction_leaf=0.0\n",
    "# max_features=None\n",
    "# random_state=None\n",
    "# max_leaf_nodes=None\n",
    "# min_impurity_split=1e-07\n",
    "# class_weight=None\n",
    "# presort=False\n",
    "\n",
    "# Define train/test indices\n",
    "skf = StratifiedKFold(OUTPUT,n_folds=3)\n",
    "skf.shuffle = 'True'\n",
    "# Initialize lists\n",
    "avgf1 = []\n",
    "f1 = []\n",
    "error = []\n",
    "idx = 0\n",
    "averageError = 0.0\n",
    "\n",
    "start = timer()\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = INPUTS[train_index,:], INPUTS[test_index,:]\n",
    "    Y_train, Y_test = OUTPUT[train_index], OUTPUT[test_index]\n",
    "    # Train Classifier\n",
    "    clf = tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=5, min_samples_split=2,\n",
    "                                      min_samples_leaf=5)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    # Test Classifier\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    # Measure Classification Error\n",
    "    # zero_one_loss returns fractions of misclasssifications (0 is perfect)\n",
    "    f1.append(f1_score(Y_test,Y_pred,average=None))\n",
    "    avgf1.append(f1_score(Y_test,Y_pred,average='micro'))    \n",
    "    error.append(zero_one_loss(Y_pred,Y_test))\n",
    "    print(\"[Fold {0}] Global F1: {1:.5f}, zero one loss: {2:.5f}\".format(idx, avgf1[idx], error[idx]))\n",
    "    print(\"Depth: {0}, Node Count: {1}\".format(clf.tree_.max_depth,clf.tree_.node_count))\n",
    "    print('\\n'.join('{0}: {1:.5f}'.format(*k) for k in enumerate(f1[idx])))\n",
    "    idx +=1\n",
    "end = timer()\n",
    "print(\"---------------------------------Computational time: %4.2f%s -------------------------------\" % (end-start,' seconds'))\n",
    "print(\"Average error: %.5f%s\" % (np.mean(error),'%'))\n",
    "print(\"Average Global F1: %.5f%s\" % (np.mean(avgf1),'%'))\n",
    "print(\"Average F1:\")\n",
    "print('\\n'.join('{0}: {1:.5f}'.format(*k) for k in enumerate(np.vstack(f1).mean(axis=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For INPUTS = []'HeadToThorax_mm','Corpulence_mm','Dist'] :\n",
    "# If max_depth = None, then avg. tree depth is 48 with ~45000 nodes (pretty bad overfitting)\n",
    "# If max_depth = 5, avg # of nodes is 55 (pretty bad overfitting)\n",
    "\n",
    "# More importantly, there are very few instances of certain labels (activities):\n",
    "a = ['b', 'bc', 'f', 'fd', 'go', 'i', 'og', 'sg', 't', 'wi', 'wo']\n",
    "num_rows =  len(df.Activity)\n",
    "\n",
    "for activity in df.Activity.unique():\n",
    "    temp = df.loc[df['Activity'] == activity]\n",
    "    print('Activity {0}: {1} rows = {2:.2f}%'.format(a[int(activity)],len(temp),100*(float(len(temp))/num_rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " len(df.Activity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
