{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into Training/Validation/Testing sets\n",
    "In order to build a good model, we need to quantify what \"good\" means. The usual way to do this is to split the data into training, validation, and testing data sets.\n",
    "\n",
    "- Training Set (used to estimate the model coefficients)\n",
    "We use the training data set to build our model. This means that in a regression problem, we use the training data set to estimate the $c_0$ and $c_1$ coefficients. In a KNN, we use the training data set to find the centroids and borders for the different categories.  \n",
    "- Validation Set (used to minimize overfitting of the training set)\n",
    "Once we have a model, we want to avoid overfitting. Overfitting refers to having a model that matches the training data very well but that it doesn't match any other data set that well. If this is the case, it means that our model parameters are too specific to the data we used to the training data. We use the validation data set to see how well the model fits new data.\n",
    "\n",
    "- Testing Set (used to test how good the model is)\n",
    "Once we are satisfied with our model parameters, we can test how well our model can predict new values by passing the testing data set into the model.\n",
    "\n",
    "There are two competing concerns when splitting the data: with less training data, your model parameter estimates will vary a lot if given a different training data set. With less testing data, your performance statistic will vary a lot depending on what is on the testing data. Broadly speaking you should be concerned with dividing data such that these two concerns are minimized. The more data you have to train and to test, the better.\n",
    "\n",
    "## k-fold cross-validation\n",
    "A powerful and common approach that combines training and validation is called $k$-fold cross-validation. In $k$-fold cross-validation, the original data set is randomly split into a training and testing data sets. A good rule-of-thumb is to have 80% of the data in the training data set and 20% in the testing data set.\n",
    "\n",
    "Then, the training data set is split into $k$ equal sized data sets. One of these $k$ data sets becomes the validation data set while the remaining $k-1$ data sets are combined into a new training data set. This results in a set of estimated model parameters and a performance metric of how the model fits the validation data set. \n",
    "\n",
    "The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data set. The k results can then be averaged to produce a single set of estimated parameters that hopefully performs better than any of the k individual results.\n",
    "\n",
    "For example, setting $k = 2$ results in 2-fold cross-validation. In 2-fold cross-validation, we randomly shuffle the dataset into two sets $D_0$ and $D_1$, so that both sets are equal size. We first build our model using $D_0$ and validate on $D_1$, followed by building the model on $D_1$ and validating on $D_0$. The resulting two sets of model parameters are then averaged to produce the final set of model parameters. A good value to try at first is to set $k=10$.\n",
    "### Stratified k-fold cross-validation\n",
    "In stratified $k$-fold cross-validation, the $k$ data sets are selected so that the mean response value is approximately equal in all them. In the case of binary classification, this means that each of the $k$ data sets contains roughly the same fraction of the positive (i.e., 1) and negative (i.e., 0) entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing and Results Visualization\n",
    "\n",
    "Once we have performed the cross-validation and we have our final model, we can input the testing data set into the model and compare how well the model outcomes match the actual testing data outcomes.\n",
    "\n",
    "### $R^2$ statistic (regression)\n",
    "The $R^2$ statistic is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression.\n",
    "\n",
    "The definition of R-squared is fairly straight-forward; it is the percentage of the response variable variation that is explained by a linear model. Or:\n",
    "\n",
    "$R^2$ = Explained variation / Total variation\n",
    "\n",
    "$R^2$ is always between 0 and 100%: 0% indicates that the model explains none of the variability of the response data around its mean. 100% indicates that the model explains all the variability of the response data around its mean.\n",
    "In general, the higher the R-squared, the better the model fits your data. However, having a low $R^2$ does not mean that the model is a bad one. Models that attempt to predict human behavior usually have low $R^2$ values because humans tend to exhibit a lot of variability in behavior. A low $R^2$is most problematic when you want to produce predictions that are precise.\n",
    "\n",
    "### Accuracy, Precision, and Recall (classification)\n",
    "\n",
    "#### Accuracy\n",
    "Accuracy measures how many predictions from our model were correct. Accuracy can be computed by dividing the number of correct predictions over the total number of predictions.\n",
    "\n",
    "For binary classification, accuracy can also be calculated in terms of true and false positive and negatives:\n",
    "- True positive (TP): the prediction was positive $Y=1$ and the real outcome was also positive$Y=1$.\n",
    "- False positive (FP): the prediction was positive $Y=1$ but the real outcome was negative $Y=0$.\n",
    "- True negative (TN): the prediction was negative $Y=0$ and the real outcome was also negative $Y=0$.\n",
    "- False negative (FN): the prediction was negative $Y=0$ but the real outcome was positive $Y=1$.\n",
    "\n",
    "$$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$$\n",
    "\n",
    "#### Precision\n",
    "Precision measures how many of the positive predictions were actually correct. This provides information on how much we can trust a positive prediction.\n",
    "\n",
    "$$Precision = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "#### Recall\n",
    "Recall is similar to prediction, but instead of asking how much we can trust the positive predictions, it asks what is the proportion of actual positives that was correctly predicted?\n",
    "\n",
    "$$Recall = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "To fully evaluate the effectiveness of a classifier model, you must examine both precision and recall. Unfortunately, preciiosn and recall are often in tension. That is, improving precision typically reduces recall. In other words, if our model tries to reduce False Positives (FP) (the model is cautious about making errors when making positive predictions), then it will tend to increase the number of False Negatives (FN) (the model will predict some values are false when they are actually positive).\n",
    "\n",
    "#### Receiver Operating Characteristic (ROC) Curve (classification)\n",
    "Recall that some classification models, like logistic regression, outputs the probability of the outcome being positive ($p(Y)$) rather than the predicted value of the outcome. Therefore, to make a prediction about the outcome $Y$ for a new inout $X$, we must choose a classification threshold value $t$ such that if $p(Y) \\leq t$ we make a negative prediction $Y=0$ and if $p(Y) > t$ we make a positive prediction $Y=1$. An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. \n",
    "\n",
    "This curve plots the false positive rate vs. the true positive rate. The False Positive Rate (FPR) measures how many false positives we got as a fraction of the total number of true negatives (False Positives (FP) + True Negatives (TN)),\n",
    "$$FPR = \\frac{FP}{FP+TN}$$\n",
    "while the True Positive Rate (TPR) is also called recall and measures how many true positives we got as a fraction of the total number of true positives (True Positives + False Negatives(FN))\n",
    "$$TPR = \\frac{TP}{TP+FN}$$.\n",
    "\n",
    "An ROC curve plots TPR vs. FPR at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives. The following figure shows a typical ROC curve.\n",
    "#### Area under ROC curve (AUC)\n",
    "AUC stands for \"Area under the ROC Curve.\" AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example. AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "y_score = reg.predict(X_test)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='r',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
