{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Workflow\n",
    "\n",
    "This notebook is meant to describe the basic steps to follow when doing a data science project. However this is not meant to be exhaustive but to provide a basic structure that you can follow for your project. Furthermore, keep in mind that while the workflow suggests that some steps come before others, data science projects are usually not this straightforward. For example, this notebook suggests that you have to define the question you want to answer and then build a model that will help you answer the question. However, you might have to rephrase the question once you have spent some time exploring the data, or you might realize that your model is not great and you need to do more data exploration or that you should change the type of model that you want to use. In summary, solving data science problems usually require many iterations of the basic workflow we present in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement and Hypothesis\n",
    "\n",
    "### Define Problem: Inference vs Prediction\n",
    "The first thing you need to do is to define what is the question you are going to try and answer. The right question to ask will depend on the types of models at your disposal and the type of questions these models can answer. For the regression and classification models we have been studying, it makes sense to ask questions that fall into the following two frameworks:\n",
    "\n",
    "**Inference:** Given a set of data you want to explain how the output is generated as a function of the data.\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. Does the number of books in the household when the child was born affect their future GPA?\n",
    "2. Assume the answer to question 1 above is positive. By how much does GPA increase per every additional book in the household when the child was born? \n",
    "3. What is the effect of the mother's education level and whether the parents are married on material hardship?\n",
    "4. Are parents that tend not to respond to interview questions more likely to be unemployed? How much more or less likely?\n",
    "5. Do children that are not very sociable tend to exhibit higher or lower levels of grit?  \n",
    "\n",
    "**Prediction** Given a new measurement, you want to use an existing data set to build a model that reliably predicts the correct value or category from a set of outcomes.\n",
    "\n",
    "1. Can we predict the child's language skills based on the number of books in the household when the child was born?\n",
    "2. Can we predict if parents have received job training based on their age?\n",
    "3. Can we predict who has been evicted based on whether the father has ever been in jail and whether the mother suffers from alcohol related issues?\n",
    "4. Can we predict who's parents are married based on the household income and the number of kids in the household?\n",
    "\n",
    "#### Correlation vs Causation! How Inference cares about causation and Prediction only cares about correlation\n",
    "Correlation says that there is a relationship between some quantity $X$ and some quantity $Y$, while causation says that $X$ causes $Y$. THIS IS NOT THE SAME THING! Check out [this webpage](http://www.tylervigen.com/spurious-correlations) for funny examples of things that are strongly correlated but that make no sense for one of them to cause the other, like number of people who drown in pools vs number of movies Nicolas Cage appears in!\n",
    "\n",
    "Inference problems postulate that there are a set of features (let's call them $W_i$ with $i=1,\\dots,n$) that cause $Y$. However, most of the times the $W_i$ features that cause $Y$ are very abstract and cannot be directly observed. For example, assume $Y$ is quality of life in a country. The $W_i$ features in this case might be things like income inequality, well-being, safety, community and social relationships, etc. which are very hard to measure because they are not well-defined quantities. \n",
    "\n",
    "What can we do then if we cannot measure the $W_i$? The answer is to measure some other set of features $X_i$ that we believe are good approximations for the $W_i$. For example, a value for income inequality can be approximated by measuring the average difference between the highest and lowest wages across corporations, and a value for safety can be approximated by measuring the number of reported robberies in an area. \n",
    "\n",
    "Exercise: can you describe why these quantities are just approximations? do wages contain all the information you need to know about income inequality? how about vacation days? how about employer provided health insurance? \n",
    "\n",
    "Once we have some $X_i$ features that we can measure, we can then collect data and build a model, such as a linear regression model to try and understand the effect of the $X_i$ on $Y$, and since we know the $X_i$ are good approximations for the $W_i$, the results can be used to understand the relationship between the $W_i$ and $Y$.\n",
    "\n",
    "Note that picking a model gives rise to a similar problem. We do not know what the \"true model\" (let's call it $F$) is that describes the relationship between $Y$ and $W_i$ (i.e. $Y = F(W_i)$), so we try and estimate a model (let's call it $f$) of the form $Y = f(X_i)$ that approximates $F(W_i)$, such that the error $F(W_i)-f(X_i)$ is small.\n",
    "\n",
    "On the other hand, the prediction problem does not care about what causes $Y$. All it cares about is whether we can predict what $Y$ will be based on the available data $X_i$. In the example above, the prediction problem does not care about income inequality per se, but it cares about whether we can use the average difference in wages in corporations to predict quality of life. \n",
    "\n",
    "### Define Success Metrics\n",
    "How will we know if we have successfully answered the inference/prediction question? \n",
    "\n",
    "For inference, this is tricky because we need to estimate how well the features we can measure $X_i$ relate to the $W_i$ features we cannot measure but that we believe cause $Y$. We cannot get a definitive answer for this, but we can argue for it. What we can do is to measure how confident we are that our model captures the effects of the predictors $X_i$ on the outcome $Y$. This at least will tell us that we did the best we can with the model. \n",
    "\n",
    "For predictions, it means we need to measure how well we can predict new values given the features $X_i$. This is easier to measure as we can simply pass a new data set to the model and see how accurate our predictions are. \n",
    "\n",
    "In both the inference and the prediction cases it is helpful if we split the data set into what is known as a training, a validation, and a testing data set. We use the training data set to find the model parameters. We then use the validation data set to see how certain we can be that the model parameters we found are correct. Finally we use the testing data set to see how well our model can predict the outcome of new observations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Once we have an idea of the question we are trying to answer, we need to take a look at the data set to see if it contains enough information to answer our question. This means we need to look for features that we believe might help us build a good model. For inference problems we try and find features in the data $X_i$ that we believe are strongly related to features not in the data $W_i$ that might be causing the outcome $Y$. For prediction we try and find good features in the data $X_i$ that we believe might be good predictors for the outcome $Y$.\n",
    "\n",
    "### Data Overview\n",
    "- How many good features related to our question are there in the data?\n",
    "- Are these features missing a lot of entries? Can we impute these missing values in a smart way?\n",
    "\n",
    "### Data Visualization\n",
    "- For continuous/discrete features: Are there a lot of outliers? Is there a well-defined mean?\n",
    "- For categorical features: Are all categories equally represented?\n",
    "- Does there appear to be a correlation between our selected features and the outcome?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "### What type of model should we use?\n",
    "The type of model we should use depends on the type of outcome we are trying to infer/predict. For continuous or discrete variables that are finely discretized (i.e., that can take on many different values), we can use linear regression or multiple linear regression models. For categorical variables, we can use classification models such as logistic regression, KNN, and decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Feature engineering refers to creating new features by combining existing features. For example, you might have found 5 different binary features that you want to use. You could then combine these 5 features into one single feature by adding them all together. This way your new feature takes on values from 0 to 5 and it might be more strongly correlated to the outcome than any of the individual 5 features is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into Training/Validation/Testing sets\n",
    "In order to build a good model, we need to quantify what \"good\" means. The usual way to do this is to split the data into training, validation, and testing data sets.\n",
    "\n",
    "- Training Set (used to estimate the model coefficients)\n",
    "We use the training data set to build our model. This means that in a regression problem, we use the training data set to estimate the $c_0$ and $c_1$ coefficients. In a KNN, we use the training data set to find the centroids and borders for the different categories.  \n",
    "- Validation Set (used to minimize overfitting of the training set)\n",
    "Once we have a model, we want to avoid overfitting. Overfitting refers to having a model that matches the training data very well but that it doesn't match any other data set that well. If this is the case, it means that our model parameters are too specific to the data we used to the training data. We use the validation data set to see how well the model fits new data.\n",
    "\n",
    "- Testing Set (used to test how good the model is)\n",
    "Once we are satisfied with our model parameters, we can test how well our model can predict new values by passing the testing data set into the model.\n",
    "\n",
    "There are two competing concerns when splitting the data: with less training data, your model parameter estimates will vary a lot if given a different training data set. With less testing data, your performance statistic will vary a lot depending on what is on the testing data. Broadly speaking you should be concerned with dividing data such that these two concerns are minimized. The more data you have to train and to test, the better.\n",
    "\n",
    "## k-fold cross-validation\n",
    "A powerful and common approach that combines training and validation is called $k$-fold cross-validation. In $k$-fold cross-validation, the original data set is randomly split into a training and testing data sets. A good rule-of-thumb is to have 80% of the data in the training data set and 20% in the testing data set.\n",
    "\n",
    "Then, the training data set is split into $k$ equal sized data sets. One of these $k$ data sets becomes the validation data set while the remaining $k-1$ data sets are combined into a new training data set. This results in a set of estimated model parameters and a performance metric of how the model fits the validation data set. \n",
    "\n",
    "The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data set. The k results can then be averaged to produce a single set of estimated parameters that hopefully performs better than any of the k individual results.\n",
    "\n",
    "For example, setting $k = 2$ results in 2-fold cross-validation. In 2-fold cross-validation, we randomly shuffle the dataset into two sets $D_0$ and $D_1$, so that both sets are equal size. We first build our model using $D_0$ and validate on $D_1$, followed by building the model on $D_1$ and validating on $D_0$. The resulting two sets of model parameters are then averaged to produce the final set of model parameters. A good value to try at first is to set $k=10$.\n",
    "### Stratified k-fold cross-validation\n",
    "In stratified $k$-fold cross-validation, the $k$ data sets are selected so that the mean response value is approximately equal in all them. In the case of binary classification, this means that each of the $k$ data sets contains roughly the same fraction of the positive (i.e., 1) and negative (i.e., 0) entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing and Results Visualization\n",
    "\n",
    "Once we have performed the cross-validation and we have our final model, we can input the testing data set into the model and compare how well the model outcomes match the actual testing data outcomes.\n",
    "\n",
    "### $R^2$ statistic (regression)\n",
    "The $R^2$ statistic is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression.\n",
    "\n",
    "The definition of R-squared is fairly straight-forward; it is the percentage of the response variable variation that is explained by a linear model. Or:\n",
    "\n",
    "$R^2$ = Explained variation / Total variation\n",
    "\n",
    "$R^2$ is always between 0 and 100%: 0% indicates that the model explains none of the variability of the response data around its mean. 100% indicates that the model explains all the variability of the response data around its mean.\n",
    "In general, the higher the R-squared, the better the model fits your data. However, having a low $R^2$ does not mean that the model is a bad one. Models that attempt to predict human behavior usually have low $R^2$ values because humans tend to exhibit a lot of variability in behavior. A low $R^2$is most problematic when you want to produce predictions that are precise.\n",
    "### Receiver Operating Characteristic (ROC) Curve (classification)\n",
    "Recall that a classification models outputs the probability of the outcome being positive ($p(Y)$). Therefore, to make a prediction about the outcome $Y$ for a new inout $X$, we must choose a classification threshold value $t$ such that if $p(Y) \\leq t$ we make a negative prediction $Y=0$ and if $p(Y) > t$ we make a positive prediction $Y=1$. An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. \n",
    "\n",
    "This curve plots the false positive rate vs. the true positive rate. The False Positive Rate (FPR) measures how many false positives we got as a fraction of the total number of true negatives (False Positives (FP) + True Negatives (TN)),\n",
    "$$FPR = \\frac{FP}{FP+TN}$$\n",
    "while the True Positive Rate (TPR) is also called recall and measures how many true positives we got as a fraction of the total number of true positives (True Positives + False Negatives(FN))\n",
    "$$TPR = \\frac{TP}{TP+FN}$$.\n",
    "\n",
    "An ROC curve plots TPR vs. FPR at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives. The following figure shows a typical ROC curve.\n",
    "#### Area under ROC curve (AUC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC stands for \"Area under the ROC Curve.\" AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example. AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Future Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Results\n",
    "\n",
    "Summarize and Report your findings:\n",
    "- What was the question you were trying to answer?\n",
    "- What model did you use to answer the questions?\n",
    "- What features did you use?\n",
    "- What procedure did you follow to build your model? (training/testing percentage split, number of k-folds, etc.)\n",
    "- What was the performance of your model? (ROC curve, AUC, $R^2$ value)\n",
    "- What are the model parameter values and what do they mean? (positive/negative correlations between $X_i$ and $Y$)\n",
    "- What is the interpretation of the results? \n",
    "    - Do your results make sense?\n",
    "    - Do you think the correlations you found imply causation? Why or why not?\n",
    "        - Do you think that the features you used are directly related to the outcome?\n",
    "        - Did you want to include other features but they were not in the data? How would these ghost features change your results?\n",
    "        - Where the features you used collected in a thorough and unbiased manner?\n",
    "        - Did you have enough data to build an accurate model?\n",
    "        - Do your model parameters change by much when you train the model with different data?\n",
    "- What could have improved your methodology and results?\n",
    "    - Better features?\n",
    "    - More data?\n",
    "    - Better models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Implications\n",
    "\n",
    "- Can you use your results to propose a policy that would help prevent or increase the probability of certain outcomes? \n",
    "    - Be very careful about correlations vs. causation when thinking about this. \n",
    "        - Do you think the correlations you found imply causation? Why or why not?\n",
    "        - Is more data needed before drawing conclusions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
